---
title: "Journal (reproducible report)"
author: "Tobias Tiedemann"
date: "2020-11-21"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    code_folding: show
---
<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #2DC6D6;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```
# Journal

## Chapter 2

Last compiled: `r Sys.Date()`

The sales data of a company in the bicycle sales sector were analyzed for the period from 2015 to 2019 and the results were presented in diagrams.
The sales in different categories were considered, as well as the sales that were made across the Federal Republic of Germany. These sales were shown across states and cities.

```{r plot,fig.width=10, fig.height=7}

# 1.0 Load libraries ----
library(tidyverse)
library(readxl)
library(lubridate)
library(writexl)

# 2.0 Importing Files ----
bikes_tbl      <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel("DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops_tbl  <- read_excel("DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
# 3.0 Examining Data ----


# 4.0 Joining Data ----
bike_orderlines_joined_tbl <- orderlines_tbl %>%
  left_join(bikes_tbl, by = c("product.id" = "bike.id")) %>%
  left_join(bikeshops_tbl, by = c("customer.id" = "bikeshop.id"))

#6.3 Sales by State
sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%
  select(order_date, total_price, state) %>%
  mutate(year = year(order_date)) %>%
  group_by(year, state) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

sales_by_state_tbl %>%
  ggplot(aes(x = year, y = sales, fill = state)) +
  geom_col() +
  facet_wrap(~ state) +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and States",
    subtitle = "",
    fill = "" # Changes the legend name
    + theme(axis.text.x = element_text(angle = 45, hjust = 1))
    )
#6.4 Sales by City
sales_by_city_tbl <- bike_orderlines_wrangled_tbl %>%
  select(order_date, total_price, city) %>%
  mutate(year = year(order_date)) %>%
  group_by(year, city) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".", 
                                     decimal.mark = ",", 
                                     prefix = "", 
                                     suffix = " €"))

sales_by_city_tbl %>%
  ggplot(aes(x = year, y = sales, fill = city)) +
  geom_col() +
  facet_wrap(~ city) +
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(
    title = "Revenue by year and Cities",
    subtitle = "",
    fill = "" # Changes the legend name
    + theme(axis.text.x = element_text(angle = 45, hjust = 1))
    )
```
# Chapter 3
Last compiled: `r Sys.Date()`

Data Aquisition via an API.
I used the Quandl Data Provider to get access to their API. The Data is the Electricity generation in Germany from 1985 until  The Data is presented as a list with the Year in one column and the produced electricity in gigawatthours in the second column.
```{r,eval=FALSE}
library(httr)
library(jsonlite)
library(tidyverse)

resp <- GET("https://www.quandl.com/api/v3/datasets/BP/ELEC_GEN_DEU.json?api_key=---------------")

resp %>% 
  .$content %>% 
  rawToChar() %>% 
  fromJSON()
resp  
```
```
dataset data
      [,1]         [,2]          
 [1,] "2018-12-31" "648.7"       
 [2,] "2017-12-31" "653.6547"    
 [3,] "2016-12-31" "650.70734396"
 [4,] "2015-12-31" "648.14928057"
 [5,] "2014-12-31" "627.76510177"
 [6,] "2013-12-31" "638.73"      
 [7,] "2012-12-31" "630.149"     
 [8,] "2011-12-31" "613.070326"  
 [9,] "2010-12-31" "633.093"     
[10,] "2009-12-31" "595.624"     
[11,] "2008-12-31" "640.686"     
[12,] "2007-12-31" "640.578"     
[13,] "2006-12-31" "639.5665"    
[14,] "2005-12-31" "622.575"     
[15,] "2004-12-31" "617.4652"    
[16,] "2003-12-31" "608.78"      
[17,] "2002-12-31" "586.688"     
[18,] "2001-12-31" "586.4115"    
[19,] "2000-12-31" "576.556"     
[20,] "1999-12-31" "556.301"     
.....  
```
# Chapter 4
 Last compiled: `r Sys.Date()`
 
 For the solution of Chapter 4 i preferred the dplyr package. At first all the data neets to be imported. I used the vroom package for doing this.
```{r,eval=FALSE}
library(vroom)
library(tidyverse)
library(lubridate)
library(data.table)

#import assignee Table
col_types_assignee <- list(
  id = col_character(),
  type = col_double(),
  name_first = col_character(),
  name_last = col_character(),
  organization = col_character()
)

assignee_tbl <- vroom(
  file       = "assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types_assignee,
  na         = c("", "NA", "NULL")
)
#import patent_assignee table
col_types_patent_assignee <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()
)

patent_assignee_tbl <- vroom(
  file       = "patent_assignee.tsv", 
  delim      = "\t", 
  col_types  = col_types_patent_assignee,
  na         = c("", "NA", "NULL")
)

#import patent table
col_types_patents <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

patent_tbl <- vroom(
  file       = "patent.tsv", 
  delim      = "\t", 
  col_types  = col_types_patents,
  na         = c("", "NA", "NULL")
)

#import uspc table
col_types_uspc <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_double()
)

uspc_tbl <- vroom(
  file       = "uspc.tsv", 
  delim      = "\t", 
  col_types  = col_types_uspc,
  na         = c("", "NA", "NULL")
)


```
After importing the Data. It needs to be reshaped and joint. Because I need som colums of every dataset but not all of them for the Solution.

The first task is to find out which company has the most patents in the US.
```{r,eval=FALSE}
# first attend with dplyr
#sort the patent table by the assignees with the most patents
patent_summary_tbl <- patent_assignee_tbl %>% select(patent_id,assignee_id)

most_patents <- patent_summary_tbl %>% 
  group_by(assignee_id) %>%
  summarise(
    count = n()
  ) %>%
  ungroup() %>%
  arrange(desc(count))

assignee_summary_tbl<-assignee_tbl %>% select(id,organization)

#save only the first 5000 assignees with the most patents ant join the Companies
#to the assignee IDs
most_patents <- most_patents %>%
  slice(1:(nrow(.)/100)) %>%
  left_join(y=assignee_summary_tbl,  by = c("assignee_id"="id"))
most_patents %>% select(count,organization)

```
List of the companies with the most total patents:
```
 count organization                               
    <int> <chr>                                      
1 139091 International Business Machines Corporation
2  93561 Samsung Electronics Co., Ltd.              
3  75909 Canon Kabushiki Kaisha                     
4  54342 Sony Corporation                           
5  49442 Kabushiki Kaisha Toshiba                   
6  47121 General Electric Company                   
7  45374 Hitachi, Ltd.                              
8  42156 Intel Corporation                          
9  37196 Fujitsu Limited                            
10  35572 Hewlett-Packard Development Company, L.P.  
```

The second task is to find out which company handed in the most patents in 2019.
```{r,eval=FALSE}
patent_summary_2019 <- patent_tbl %>% 
  select(id,date) %>%
  mutate(year=year(date)) %>%
  mutate(amount=1)%>%
  left_join(y=patent_summary_tbl,  by = c("id"="patent_id")) %>%
  left_join(y=assignee_summary_tbl,  by = c("assignee_id"="id"))
glimpse(patent_summary_2019)

class(patent_summary_2019)
setDT(patent_summary_2019)
list_2019 <- patent_summary_2019[year == 2019,sum(amount),organization] 
list_2019 %>% arrange(desc(V1))


```
List of the companies with the most patents in 2019:
```
 count organization                               
    <int> <chr>                                      
1 28824 NA
2 9265 International Business Machines Corporation              
3 7205 Samsung Electronics Co., Ltd.                     
4 3595 Canon Kabushiki Kaisha                           
5 3526 Canon Intel Corporation      
```

For the 28824 patents in 2019 wit "NA" it has to be mentioned that these are private persons who admitted the patents who dont relate to a specifiv company.

The third task is to find out the most innovative tech sectors by the amount of patents in the specific sector. 

```{r,eval=FALSE}
uspc_mainclasses <-uspc_tbl %>%
  select(patent_id,mainclass_id) %>%
  left_join(y=patent_summary_tbl, by = c("patent_id"="patent_id")) %>%
  left_join(y=assignee_summary_tbl,  by = c("assignee_id"="id")) %>%
  group_by(mainclass_id) %>%
  summarise(
    count = n()
  ) %>%
  ungroup() %>%
  arrange(desc(count))
```
The Output of the uspc_mainclass list is the following:
```
mainclass_id  count
chr         int
1 257 450431
2 428 443985
3 435 391347
4 514 344342
5 424 297210
``` 
If you look up the mainclass IDs on the uspto website you can find out, that the most innovative tech secors are:
 
```
1. 257 - Active solid-state devices (e.g., transistors, solid-state diodes)
2. 428 - Stock material or miscellaneous articles
3. 435 - Chemistry: molecular biology and microbiology
4. 514 - Drug, bio-affecting and body treating compositions
5. 424 - Drug, bio-affecting and body treating compositions
```
 There is also a note that 514 is an integral Part of 424 which would probably effect the ranking.

# Chapter 5

The first task of chapter 5 is to deal with the ggplot2 package and plot the cummulative Covid19 cases in 2020 for some countries. The theme, the colours and the labels of the plot have been edited.

```{r}
library(tidyverse)
library(lubridate)
library(data.table)
library(scales)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")
  

data_tbl <- covid_data_tbl %>% 
  distinct(cases, dateRep, countriesAndTerritories) %>% 
  filter(countriesAndTerritories == 'Germany' | 
           countriesAndTerritories == 'United_Kingdom' | 
           countriesAndTerritories == 'Spain' | 
           countriesAndTerritories == 'France') %>%
  mutate(date       = lubridate::dmy(dateRep)) %>% 
  arrange(date) %>%
  group_by(countriesAndTerritories) %>%
  mutate(sum_cases = cumsum(cases)) %>%
  ungroup


p <- ggplot(data=data_tbl,aes(x=date,y=sum_cases,color=countriesAndTerritories))+
  geom_line(size=2) +
  scale_color_manual(values=c('#0072BD','#D95319','#EDB120','#7E2F8E'))+
  scale_x_date(labels=date_format("%Y-%m"),breaks=date_breaks("1 months"))

p + labs(title = "Cummulative Covid cases in european countries",
         caption = "Number of Covid Cases is still growing fast",
         x = "Date",
         y = "Cummulative Covid Cases per country",
         color = "Country" )+
  annotate("text",
           x=date(c('2020-11-20','2020-12-01','2020-12-01','2020-11-01')),
           y=(c(400000,1300000,1700000,2200000)),
           label=(c("Germany","Spain","UK","France")),
           color=c('#D95319','#EDB120','#7E2F8E','#0072BD'))+

theme(
     axis.text.x = element_text(
       angle = 45,
       hjust = 1
     ),
     strip.text = element_text(
       face  = "bold",
       color = '#0072BD'
     )
   )
```

The second task is to plot the mortality rate of the covid19 virus devided by countries on a world map.

```{r}
library(scales)
world_map <- map_data("world")

covid_worldwide_tbl <- covid_data_tbl %>%
  select(countriesAndTerritories,deaths,popData2019,dateRep)%>%
  mutate(across(countriesAndTerritories, str_replace_all, "_", " "))%>%
  mutate(countriesAndTerritories = case_when(
  countriesAndTerritories == "United Kingdom" ~ "UK",
  countriesAndTerritories == "United States of America" ~ "USA",
  countriesAndTerritories == "Czechia" ~ "Czech Republic",
  TRUE ~ countriesAndTerritories
)) %>%
  mutate(date = lubridate::dmy(dateRep)) %>% 
  arrange(date) %>%
  group_by(countriesAndTerritories) %>%
  mutate(toal_deaths = cumsum(deaths)) %>%
  filter(date=="2020-12-01") %>%
  mutate(mortality=toal_deaths/popData2019) %>%
  left_join(y=world_map, by =c("countriesAndTerritories"="region"))
 
theme_set(
  theme_void()
)

ggplot(covid_worldwide_tbl, aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill=mortality), colour = "grey")
```